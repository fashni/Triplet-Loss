{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JHDnuxrBet8"
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/fashni/triplet-loss.git\n",
        "%cd triplet-loss\n",
        "!pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CTnDwUmBevC"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfW185X5BevT"
      },
      "outputs": [],
      "source": [
        "# LFW-df\n",
        "!wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\n",
        "!wget https://media.fashni.space/pub/dataset/lfw-deepfunneled/train.json\n",
        "!wget https://media.fashni.space/pub/dataset/lfw-deepfunneled/valid.json\n",
        "!tar -xzf lfw-deepfunneled.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddDwXGPXBeve"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP4KKHsFBeyD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from dataset import BatchGenerator, TripletGenerator\n",
        "from models import facenet, inception, inception_resnet, siamnet\n",
        "from utils import (get_embeddings, get_images_and_labels, get_metrics,\n",
        "                   get_pairwise_similarity)\n",
        "\n",
        "models = {\n",
        "    \"facenet\": facenet,\n",
        "    \"inception\": inception,\n",
        "    \"inception_resnet\": inception_resnet,\n",
        "    \"siamnet\": siamnet\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fs8DzTaaBeyI"
      },
      "outputs": [],
      "source": [
        "seed = 69\n",
        "\n",
        "# Dataset Parameters\n",
        "train_path = 'train.json'\n",
        "valid_path = 'valid.json'\n",
        "batch_size = 32\n",
        "augment = False\n",
        "dset_name = \"lfw\"\n",
        "\n",
        "# Model Parameters\n",
        "input_shape = (160, 160, 3)\n",
        "embedding_size = 128\n",
        "model_name = \"facenet\"\n",
        "\n",
        "# Training Parameters\n",
        "learning_rate = 0.0001\n",
        "epochs = 10\n",
        "\n",
        "# Loss Parameters\n",
        "strategy = \"batch_all\" # siamese, batch_all, or batch_hard\n",
        "margin = 0.5\n",
        "squared = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smpzn-KwBeyO"
      },
      "outputs": [],
      "source": [
        "# Create data generator\n",
        "DatasetGenerator = TripletGenerator if strategy == \"siamese\" else BatchGenerator\n",
        "train_dataset_generator = DatasetGenerator(train_path,\n",
        "                                           batch_size=batch_size,\n",
        "                                           input_shape=input_shape,\n",
        "                                           augment=augment, seed=seed)\n",
        "valid_dataset_generator = DatasetGenerator(valid_path,\n",
        "                                           batch_size=batch_size,\n",
        "                                           input_shape=input_shape,\n",
        "                                           augment=False, seed=seed)\n",
        "\n",
        "# Get the total images in the dataset\n",
        "n_train = train_dataset_generator.n_images\n",
        "n_valid = valid_dataset_generator.n_images\n",
        "\n",
        "# Get datasets\n",
        "train_dataset = train_dataset_generator.get_dataset()\n",
        "valid_dataset = valid_dataset_generator.get_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ee0JTD2Bey4"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "model = models[model_name](input_shape, embedding_size, margin, squared, strategy)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2Q7AjEwBey6"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, weighted_metrics=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RaNOVnjBezG"
      },
      "outputs": [],
      "source": [
        "# Train and validation steps per epoch\n",
        "train_steps = min(200, -(-n_train // batch_size))\n",
        "valid_steps = min(50, -(-n_valid // batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N44aXDiwBezI"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "hist = model.fit(\n",
        "  train_dataset.take(train_steps),\n",
        "  epochs=epochs,\n",
        "  validation_data=valid_dataset.take(valid_steps),\n",
        "  verbose=1,\n",
        "  initial_epoch=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECgtFgK4BezL"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57NDvDgXBezQ"
      },
      "outputs": [],
      "source": [
        "test_dataset = BatchGenerator(valid_path,\n",
        "                              batch_size=batch_size,\n",
        "                              input_shape=input_shape,\n",
        "                              augment=False, seed=seed).get_dataset()\n",
        "images, labels = get_images_and_labels(test_dataset, valid_steps)\n",
        "images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5liB9C-BezS"
      },
      "outputs": [],
      "source": [
        "embeddings = get_embeddings(model, images, batch_size, verbose=1)\n",
        "y_true, y_pred = get_pairwise_similarity(embeddings, labels, squared=squared, norm=False)\n",
        "fpr, tpr, prc, acc, f1, thres, auc = get_metrics(y_true, y_pred)\n",
        "j = (tpr-fpr).argmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpCXCLIkBezU"
      },
      "outputs": [],
      "source": [
        "print(f\"{auc = }\")\n",
        "print(f\"{f1[j] = }\")\n",
        "print(f\"{acc[j] = }\")\n",
        "print(f\"{prc[j] = }\")\n",
        "print(f\"{tpr[j] = }\")\n",
        "print(f\"{fpr[j] = }\")\n",
        "print(f\"{thres[j] = }\")\n",
        "\n",
        "plt.plot(fpr, tpr)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3lqOFJfBezW"
      },
      "source": [
        "# Save weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QsASsrzBezY"
      },
      "outputs": [],
      "source": [
        "model.save_weights(f\"{model_name}_{dset_name}.weights.h5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
